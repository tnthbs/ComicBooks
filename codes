import requests
import os
from lxml import etree
import time,random

#import time
def getChapterURL(mainURL):
    res= requests.get(mainURL)
    selector = etree.HTML(res.content)  #res.content对象才为HTML源码
    chpturl = selector.xpath('//*[@id="permalink"]/div[4]/ul[1]/li/a/@href')
    for chpt in chpturl:
        chpturl[chpturl.index(chpt)] = 'http://www.hheess.com/'+chpt
    return chpturl
 
def mkdir(path):
    path=path.strip()
    # 去除尾部 \ 符号
    path=path.rstrip("\\")
     # 判断路径是否存在
    # 存在     True
    # 不存在   False
    isExists=os.path.exists(path)
     # 判断结果
    if not isExists:
        # 如果不存在则创建目录
        print(path+' 创建成功')
        # 创建目录操作函数
        os.makedirs(path)
        return True
    else:
        # 如果目录存在则不创建，并提示目录已存在
        print(path+' 目录已存在')
        return False

def getContent(url,startpage):
    from selenium import webdriver
    import re
    i=startpage
    urlhead = re.search(r'http\S+/',url).group() #为构造下次循环网址准备 http://www.hheess.com//page86555/1.html?s=13 >
    #获得 http://www.hheess.com//page86555/
    
    url = urlhead+str(i)+r'.html?s=13' # http://www.hheess.com//page86555/+ i+ .html?s=13  构造start page
    
    r=requests.get(url) #获取第一次网页访问是否正常
    rcode = r.status_code
    
    while rcode == 200: #若返回值为200代表网页正常
        try:
            #'''第一次访问第一页，'''
            driver = webdriver.Chrome()#网页含有js脚本，只能通过selenium 访问
            driver.get(url)
            source = driver.page_source #获取
            picurl = re.findall('http://\S+.JPG',source) #匹配结果会得到两个，第一个为下一页网址，第二个为本页网址
            driver.close()
            # or //*[@id="iBody"]/div[1]
        except requests.exceptions.ConnectionError:
            print('当前章节已下载完毕或无法访问')
        #'''下载当前页及下一页'''
        
         
        for eachurl in picurl[::-1]:
            chptname = re.search('vol_\d+',eachurl).group()
            picname = re.search('-(\d+)',eachurl).group()
            file_name = chptname+picname+'.jpg' #拼接图片名
            print(file_name)

            try:
                pic = requests.get(eachurl)  
            except requests.exceptions.ConnectionError:
                print('当前图片无法下载')

            #将图片存入本地
            fp = open(file_name,'wb')
            fp.write(pic.content) #写入图片
            fp.close()
            time.sleep(random.uniform(0.5, 3))#下完一章图片后随机睡眠，防爬虫

        
        i=i+2 #在第一页中已获得第一页第二页的图片的网址，可以直接跳转到第三页
        writelog(url) #save current processing url
        
        url = urlhead+str(i)+r'.html?s=13' # http://www.hheess.com//page86555/+ i+ .html?s=13
        
        try:
            r=requests.get(url)   #若能访问，则继续运行
            rcode = r.status_code
            #不存在的网页页可以被访问 http://www.hheess.com//page86555/+ 999+ .html?s=13
            content=r.content.decode(encoding='utf-8')
            a= re.search('\d+</a> 页',content).group()
            a= re.search('\d+</a> 页',a).group()
            a=a.strip('</a> 页')
            a=int(a)
            if i>a:
                rcode = 0
        except:
            rcode = 0  #若新构造的页面无法访问，则赋值0，退出while循环
            print('当前章节已下载完毕或无法访问')
            
def writelog(url):
    log='log.txt'
    with open (log,'w+') as writefile:
        writefile.write(url)

def checklog():
    try:#若文件不存在时，不会报错
        with open ('log.txt') as log:
            logurl = log.readline()
            currentpage = re.search('/(\d+).html',logurl).groups()  #此时为元祖 （'11'）
            currentchpt = logurl.replace(currentpage[0]+'.html?s=13','1.html?s=13') #元组的第一个元素才能被re 表示式接收
            currentpage = int(currentpage[0]) #整形为int，后面需要运算i+2
            #currenpage 搜出来结果是tuple，tuple第一个元素str才能被sub

    except:
            currentpage = ''
            currentchpt = ''

    return currentchpt,currentpage
        

if __name__ == '__main__':
    #mkpath= input('please input the storage path \n')
    mkpath = r'F:\cb_new'
    mkdir(mkpath)
    os.chdir(mkpath) 
    #mainurl = input('please input the mainpage url \n')
    mainurl = r'http://www.hheess.com/manhua8612.html'
    chapterurl = getChapterURL(mainurl) #获取章节目录
    currentchpt,currentpage=checklog()

    if currentchpt !='': #如果已有下载记录
        getContent(currentchpt,currentpage) #从当前未下载完成的继续至下完本章节
        for chpt in chapterurl[:chapterurl.index(currentchpt):-1]: 
            #继续下载剩余章节,当前章节及当前之前的章节不再下载
            #如正在下载vol1,则是 chpapterurl[:-1:-1],从倒数第二章节（chpt2）继续下载
            currentchpt = chpt
            currentpage = 1
            getContent(currentchpt,currentpage)
    else:  #如果没有文件或没有下载记录，从第一章开始下
        for chpt in chapterurl[::-1]: #第一个元素为最新的一章，从第一章开始下载
            getContent(chpt,1)
    
