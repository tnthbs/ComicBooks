import requests
import os
from lxml import etree
import time,random

#import time
def getChapterURL(mainURL):
    res= requests.get(mainURL)
    selector = etree.HTML(res.content)  #res.content对象才为HTML源码
    chpturl = selector.xpath('//*[@id="permalink"]/div[4]/ul[1]/li/a/@href')
    for chpt in chpturl:
        chpturl[chpturl.index(chpt)] = 'http://www.hheess.com/'+chpt
    return chpturl
 
def mkdir(path):
    path=path.strip()
    # 去除尾部 \ 符号
    path=path.rstrip("\\")
     # 判断路径是否存在
    # 存在     True
    # 不存在   False
    isExists=os.path.exists(path)
     # 判断结果
    if not isExists:
        # 如果不存在则创建目录
        print(path+' 创建成功')
        # 创建目录操作函数
        os.makedirs(path)
        return True
    else:
        # 如果目录存在则不创建，并提示目录已存在
        print(path+' 目录已存在')
        return False

def getContent(url):
    from selenium import webdriver
    import re
    i=1
    urlhead = re.search(r'http\S+/',url).group() #为构造下次循环网址准备 http://www.hheess.com//page86555/1.html?s=13 >
    #获得 http://www.hheess.com//page86555/
    r=requests.get(url) #获取第一网页访问是否正常
    rcode = r.status_code
    
    while rcode == 200: #若返回值为200代表网页正常
        try:
            #'''第一次访问第一页，'''
            driver = webdriver.Chrome()#网页含有js脚本，只能通过selenium 访问
            driver.get(url)
            source = driver.page_source #获取
            picurl = re.findall('http://\S+.JPG',source) #匹配结果会得到两个，第一个为下一页网址，第二个为本页网址
            driver.close()
            # or //*[@id="iBody"]/div[1]
        except requests.exceptions.ConnectionError:
            print('当前章节已下载完毕或无法访问')
           
        #'''下载当前页及下一页'''
        for eachurl in picurl[::-1]:
            chptname = re.search('vol_\d+',eachurl).group()
            picname = re.search('-(\d+)',eachurl).group()
            file_name = chptname+picname+'.jpg' #拼接图片名
            print(file_name)

            try:
                pic = requests.get(eachurl)  
            except requests.exceptions.ConnectionError:
                print('当前图片无法下载')

            #将图片存入本地
            fp = open(file_name,'wb')
            fp.write(pic.content) #写入图片
            fp.close()
            time.sleep(random.uniform(0.5, 3))#下完一章图片后随机睡眠，防爬虫


        i=i+2 #在第一页中已获得第一页第二页的图片的网址，可以直接跳转到第三页
        try:
            r=requests.get(url)   #若能访问，则继续运行
            rcode = r.status_code
            #不存在的网页页可以被访问 http://www.hheess.com//page86555/+ 999+ .html?s=13
            content=r.content.decode(encoding='utf-8')
            a= re.search('\d+</a> 页',content).group()
            a= re.search('\d+</a> 页',a).group()
            a=a.strip('</a> 页')
            a=int(a)
            if i>a:
                rcode = 0
        except:
            rcode = 0  #若新构造的页面无法访问，则赋值0，退出while循环
            print('rcode is in except')

if __name__ == '__main__':
    mkpath= input('please input the storage path \n')
    mkdir(mkpath)
    os.chdir(mkpath) 
    mainurl = input('please input the mainpage url \n')
    chapterurl = getChapterURL(mainurl) #获取章节目录
    for chpt in chapterurl[::-1]: #第一个元素为最新的一章，从第一章开始下载
        getContent(chpt)
