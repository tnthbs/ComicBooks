import requests
import os
from lxml import etree
import time,random
import re


def getChapterURL(mainURL):
    res= requests.get(mainURL)
    selector = etree.HTML(res.content)  #res.content对象才为HTML源码
    chpturl = selector.xpath('//*[@id="permalink"]/div[4]/ul[1]/li/a/@href')
    for chpt in chpturl:
        chpturl[chpturl.index(chpt)] = 'http://www.hheess.com/'+chpt
    return chpturl
 
def mkdir(path):
    path=path.strip()
    # 去除尾部 \ 符号
    path=path.rstrip("\\")
     # 判断路径是否存在
    # 存在     True
    # 不存在   False
    isExists=os.path.exists(path)
     # 判断结果
    if not isExists:
        # 如果不存在则创建目录
        print(path+' 创建成功')
        # 创建目录操作函数
        os.makedirs(path)
        return True
    else:
        # 如果目录存在则不创建，并提示目录已存在
        print(path+' 目录已存在')
        return False

def getContent(url,startpage):
    from selenium import webdriver
    import re
    i=startpage
        
    urlhead = re.search(r'http\S+/',url).group() #为构造下次循环网址准备 http://www.hheess.com//page86555/1.html?s=13 >
    #获得 http://www.hheess.com//page86555/
    url = urlhead+str(i)+r'.html?s=13' # http://www.hheess.com//page86555/+ i+ .html?s=13  构造start page
    
    r=requests.get(url) #获取第一次网页访问是否正常
    rcode = r.status_code
    
    while rcode == 200: #若返回值为200代表网页正常
        trycount =1
        while trycount <=3:
            try:
                #'''第一次访问第一页，'''
                '''update headless'''
                chrome_options = webdriver.ChromeOptions()
                chrome_options.add_argument('--headless')
                #实例化Chrome driver
                driver=webdriver.Chrome(options=chrome_options)
                #driver = webdriver.Chrome()#网页含有js脚本，只能通过selenium 访问
                '''update headless'''
                driver.get(url)
                source = driver.page_source #获取
                picurl = re.findall('http://\S+.JPG',source) #匹配结果会得到两个，第一个为下一页网址，第二个为本页网址
                maxpage = re.search('(\d+)</b>',source).groups()[0] #获取最大页码数
                driver.close()
                # or //*[@id="iBody"]/div[1]
                break
            except Exception as e:
                print('Exception: ',e)
                print('第{}次 当前章节无法访问或出错'.format(str(i)))
                trycount = trycount +1 #第一次失败，开始尝试第二次line50
        
        if i >int(maxpage):  #前一次获取maxpage ，maxpage为str，i为int
            print('当前页数不存在，超出最大页数') #加入在最后一页出错时，log里面写入的待处理页面不存在，重选加载后无法读取图片信息，正则表达式无法匹配图片
            break
        #'''下载当前页及下一页'''             
        for eachurl in picurl[::-1]:
            chptname = re.search('vol_\d+',eachurl).group()
            picname = re.search('-(\d+)|_(\d+)_',eachurl).group()   #chpt1-36前page图片格式为vol_xx\xxx-0010-xxx,chpt37- page图片名称 为 vol_xx\xxx_0010_xx
            file_name = chptname+picname+'.jpg' #拼接图片名
            print(file_name)
            try:#下载图片
                pic = requests.get(eachurl)  
            except Exception as e:
                print('Exception: ',e)
                print('图片无法下载')
            #将图片存入本地
            fp = open(file_name,'wb')
            fp.write(pic.content) #写入图片
            fp.close()
            time.sleep(random.uniform(0.1, 2))#下完一章图片后随机睡眠，防爬虫           
        
        i=i+2 #在第一页中已获得第一页第二页的图片的网址，可以直接跳转到第三页        
        url = urlhead+str(i)+r'.html?s=13' # 构造下一页网址 http://www.hheess.com//page86555/+ i+ .html?s=13
        writelog(url) #save next url needed processed
        
        #不存在的网页页可以被访问 http://www.hheess.com//page86555/+ 999+ .html?s=13
        if i>int(maxpage): #如果构造的页面超过章节的最大页数，停止循环
            rcode = 0     
            print('当前章节已下载完毕')
            break #如果下一页网址


def writelog(url):
    log='log.txt'
    with open (log,'w+',encoding='UTF-8') as writefile: #未指定编码方式时，保存和读取会出错。。。
        writefile.write(url)
        

def checklog():
    try:#若文件不存在时，不会报错
        with open ('log.txt',encoding='UTF-8') as log:  #未指定编码方式时，保存和读取会出错。。。
            logurl = log.readline()
            currentpage = re.search('/(\d+).html',logurl).groups()  #此时为元祖 （'11'）
            currentchpt = logurl.replace(currentpage[0]+'.html?s=13','1.html?s=13') #元组的第一个元素才能被re 表示式接收
            currentpage = int(currentpage[0]) #整形为int，后面需要运算i+2
            #currenpage 搜出来结果是tuple，tuple第一个元素str才能被sub
    except:
            print('logs not exist,start from chpt1 p1')
            currentpage = ''
            currentchpt = ''
    return currentchpt,currentpage



if __name__ == '__main__':
    #mkpath= input('please input the storage path \n')
    mkpath = r'F:\python_test'
    mkdir(mkpath)
    os.chdir(mkpath) 
    #mainurl = input('please input the mainpage url \n')
    #mainurl = mainurl.strip()  
    #if r'http://' not in mainurl: #输入网址时，可能只输入 www.xxxx.com ，程序只认 http://www.xxx.com
        #mainurl = r'http://'+mainurl
    mainurl = r'http://www.hheess.com/manhua8612.html'
    chapterurl = getChapterURL(mainurl) #获取章节目录
    currentchpt,currentpage=checklog()

    if currentchpt !='': #如果已有下载记录
        getContent(currentchpt,currentpage) #从当前未下载完成的继续至下完本章节   
        #此时currentchpt 会带有BOM 头
        if '\ufeff' in currentchpt:
            currentchpt = currentpage.replace('\ufeff','')
        if '&d=0' in currentchpt:
            currentchpt = currentchpt.replace('&d=0','') #访问过程会网址会加入&d=0后缀，和章节列表无法匹配
            
        for chpt in chapterurl[chapterurl.index(currentchpt)-1::-1]: 
            #继续下载剩余章节,当前章节及当前之前的章节不再下载
            #如正在下载vol_1,则是 chpapterurl[-1-1::-1],从倒数第二章节（chpt2）继续下载
            currentchpt = chpt
            currentpage = 1
            print(currentchpt)
            getContent(currentchpt,currentpage)
    else:  #如果没有文件或没有下载记录，从第一章开始下
        for chpt in chapterurl[::-1]: #第一个元素为最新的一章，从第一章开始下载
            getContent(chpt,1)
